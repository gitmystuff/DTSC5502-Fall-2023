{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gitmystuff/DTSC5502/blob/main/Module_13/Module_13_Activity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sB1y5IlEWoQv"
      },
      "source": [
        "# Module 13 Activity\n",
        "\n",
        "by Your Name\n",
        "\n",
        "## Getting Started\n",
        "\n",
        "* Colab - get notebook from gitmystuff DTSC5502 repository\n",
        "* Save a Copy in Drive\n",
        "* Remove Copy of\n",
        "* Submit shared link in Canvas\n",
        "\n",
        "## Overview\n",
        "\n",
        "* https://steemit.com/ai/@cpufronz/the-rise-and-fall-of-siraj-raval\n",
        "* Natural Language Processing\n",
        "* Types of Language Models\n",
        "* Review of Neural Nets\n",
        "  * ANNs\n",
        "  * CNNs\n",
        "  * RNNs, LSTMs, Seq2Seq, and Attention\n",
        "* Transformers\n",
        "* Hugging Face\n",
        "  * Sentiment Analysis\n",
        "  * Text Generation\n",
        "  * Masked Language Modeling\n",
        "  * Named Entity Recognition\n",
        "  * Text Summarization\n",
        "  * Translation\n",
        "  * Zero Shot Classification\n",
        "  * Fine Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKYkEhqy3FgI"
      },
      "source": [
        "## Natural Language Processing\n",
        "\n",
        "* See Natural Language Processing Notebook\n",
        "* Word Embedding - Word embedding or word vector is an approach with which we represent documents and words. It is defined as a numeric vector input that allows words with similar meanings to have the same representation. It can approximate meaning and represent a word in a lower dimensional space\n",
        "* CBOW or Skip-gram: finding context words at a time\n",
        "* Using multiple (many) activation functions for each word intuition - word and context\n",
        "* Then we use something like PCA to reduce dimensions to x and y for plotting\n",
        "* See Dimensionality Notebook\n",
        "\n",
        "Note: According to ycombinator - Word2Vec and bag-of-words/tf-idf are somewhat obsolete in 2018 for modeling. For classification tasks, fasttext (https://github.com/facebookresearch/fastText) performs better and faster.\n",
        "\n",
        "Sources\n",
        "* https://news.ycombinator.com/item?id=16224585\n",
        "* https://www.youtube.com/watch?v=viZrOnJclY0  \n",
        "* https://www.turing.com/kb/guide-on-word-embeddings-in-nlp\n",
        "* https://arxiv.org/pdf/1402.3722v1.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QoEtXnnzGyY_"
      },
      "outputs": [],
      "source": [
        "# import gensim.downloader as api\n",
        "\n",
        "# info = api.info()\n",
        "# for model_name, model_data in sorted(info['models'].items()):\n",
        "#   print(f\"{model_name} ({model_data.get('num_records', -1)}), {model_data['description'][:40]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ar8O3GW1LLug"
      },
      "outputs": [],
      "source": [
        "# wv = api.load('word2vec-google-news-300')\n",
        "# [========------------------------------------------] 16.7% 278.1/1662.8MB downloaded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_DkJVpBKsiPu"
      },
      "outputs": [],
      "source": [
        "# import nltk\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sic9I-G6HdZv"
      },
      "outputs": [],
      "source": [
        "# pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XspIDuhvGPCT"
      },
      "outputs": [],
      "source": [
        "# https://aneesha.medium.com/using-tsne-to-plot-a-subset-of-similar-words-from-word2vec-bb8eeaea6229\n",
        "# https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673\n",
        "# Dimensionality notebook\n",
        "\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# import re\n",
        "# from gensim.models import Word2Vec\n",
        "# from sklearn.decomposition import PCA\n",
        "# import wikipedia\n",
        "# from collections import Counter\n",
        "# from nltk.tokenize import sent_tokenize\n",
        "# from nltk.corpus import stopwords\n",
        "\n",
        "# stopwords = set(stopwords.words('english'))\n",
        "\n",
        "# text = wikipedia.search(\"Data Science\")\n",
        "# page = wikipedia.page(text[0])\n",
        "\n",
        "# sent_tokens = sent_tokenize(page.content)\n",
        "# data = []\n",
        "# for sent in sent_tokens:\n",
        "#   dat = re.sub('[^a-zA-Z]', ' ', sent)\n",
        "#   dat = dat.lower().split()\n",
        "#   dat = filter(lambda x: x not in stopwords, dat)\n",
        "#   dat = ' '.join(dat)\n",
        "#   data.append(dat)\n",
        "\n",
        "# sentences = [line.split() for line in data]\n",
        "# model = Word2Vec(sentences, vector_size=100, window=5, workers=4, min_count=5)\n",
        "\n",
        "# counter = Counter(sentences[0])\n",
        "# for i in sentences:\n",
        "#     counter.update(i)\n",
        "\n",
        "# # plot x, y using dimension reduction\n",
        "# def display_similarities(model, words):\n",
        "#   vectors = np.array([model.wv[word] for word in words if word in model.wv])\n",
        "#   pca_vals = PCA().fit_transform(vectors)[:, :2]\n",
        "#   x = pca_vals[:,0]\n",
        "#   y = pca_vals[:,1]\n",
        "#   plt.scatter(x, y)\n",
        "\n",
        "#   for label, (x, y) in zip(words,pca_vals):\n",
        "#     plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
        "\n",
        "# most_common = [t[0] for t in counter.most_common(10)]\n",
        "# for term in most_common:\n",
        "#   print(term)\n",
        "\n",
        "# display_similarities(model, most_common)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ST3kh8XqetTq"
      },
      "outputs": [],
      "source": [
        "# model.wv.most_similar('data', topn=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2J-M8vlIWMNG"
      },
      "outputs": [],
      "source": [
        "# from sklearn.manifold import TSNE\n",
        "\n",
        "# def display_similar_by_word(model, word, topn):\n",
        "#     close_words = model.wv.most_similar(word, topn=topn)\n",
        "#     for word in close_words:\n",
        "#       print(word)\n",
        "\n",
        "#     words = []\n",
        "#     for t in close_words:\n",
        "#       words.append(t[0])\n",
        "\n",
        "#     vectors = np.array([model.wv[word] for word in words])\n",
        "#     pca_vals = PCA().fit_transform(vectors)[:, :2]\n",
        "#     x = pca_vals[:,0]\n",
        "#     y = pca_vals[:,1]\n",
        "#     plt.scatter(x, y)\n",
        "\n",
        "#     for label, (x, y) in zip(words,pca_vals):\n",
        "#       plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')\n",
        "\n",
        "# display_similar_by_word(model, 'data', 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2DkvD0m3Sa4"
      },
      "source": [
        "## Language Models\n",
        "\n",
        "* Both word embeddings (e.g Word2Vec) and language models (e.g BERT) are ways of representing text, where language models capture more information and are considered state-of-the-art for representing natural language in a vectorized format\n",
        "* Language models are self-supervised and based on transformers\n",
        "* The language model is a word sequence prediction model trained to predict the next word based on the previous input sentence. This kind of task is a self-supervised learning task because you are not defining separate output labels\n",
        "* GPT\n",
        "* BERT\n",
        "* PaLM\n",
        "* LaMDA\n",
        "\n",
        "Source\n",
        "\n",
        "* https://ai.stackexchange.com/questions/26739/what-is-the-difference-between-a-language-model-and-a-word-embedding#:~:text=Both%20word%20embeddings%20(e.g%20Word2Vec,language%20in%20a%20vectorized%20format.\n",
        "* https://www.turing.com/kb/introduction-to-self-supervised-learning-in-nlp#:~:text=The%20language%20model%20is%20a,not%20defining%20separate%20output%20labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqYel0E03LOg"
      },
      "source": [
        "## Neural Nets Review\n",
        "\n",
        "* https://pytorch.org/tutorials/beginner/blitz/\n",
        "* LSTM\n",
        "  * RNNs have problem with memory, vanishing gradients\n",
        "  * LSTMs seek to resolve this issue with separate paths for long and short term memory\n",
        "\n",
        "<img src='https://upload.wikimedia.org/wikipedia/commons/thumb/9/93/LSTM_Cell.svg/512px-LSTM_Cell.svg.png?20210827110428' alt='lstm' />\n",
        "\n",
        "Source: Guillaume Chevalier - Schematic of the Long-Short Term Memory cell, a component of recurrent neural networks - https://en.wikipedia.org/wiki/Long_short-term_memory\n",
        "* Seq2Seq / Encoder Decoder Model\n",
        "  * Similar to an embedding layer attached to a single LSTM cell and a  context between cells\n",
        "  * Encoder Decoder Models\n",
        "\n",
        "<img src='https://i.stack.imgur.com/YjlBt.png' alt='seq2seq' width='640px' />\n",
        "\n",
        "Source: suriyadeepan.github.io and https://stackoverflow.com/questions/46355651/understanding-seq2seq-model\n",
        "\n",
        "https://arxiv.org/abs/1409.3215\n",
        "\n",
        "* Attention - https://www.youtube.com/watch?v=PSs6nxngL6k\n",
        "  * Attention allows decoder to access each input (word) from the encoder\n",
        "  * Establishes similarity scores between encoder and decoder cells (scores such as cosine similarity, or dot product)\n",
        "\n",
        "<img src='https://miro.medium.com/v2/resize:fit:720/format:webp/1*A4H-IhqwjNZ_eL57Cqch0A.png' alt='encoder decoder model with attention' width='640px' />\n",
        "\n",
        "Source - https://towardsdatascience.com/day-1-2-attention-seq2seq-models-65df3f49e263\n",
        "\n",
        "See - https://www.linkedin.com/pulse/explanation-attention-based-encoder-decoder-deep-keshav-bhandari/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL3SWdma3NhS"
      },
      "source": [
        "## Transformers\n",
        "\n",
        "https://huggingface.co/learn/nlp-course/chapter1/4\n",
        "\n",
        "It's all about attention (or context?)\n",
        "\n",
        "* The chicken crossed the road because it was hungry\n",
        "* The chicken crossed the road because it was the way home\n",
        "* Query (what is it), Key (it), Value (depends on context)\n",
        "* Attention (context) mapping provides the importance\n",
        "* chicken and hungry vs road and home\n",
        "\n",
        "Terms you may hear of\n",
        "* Self Attention: what's the next word? text generation and summarization\n",
        "* Bidirectional Encoder Representations from Transformers (BERT) is a deep learning strategy for natural language processing (NLP) that helps artificial intelligence (AI) programs understand the context of ambiguous words in text, what are the words sorrounding a word? maked language modelling\n",
        "* Positional Encoding: Positional encoding describes the location or position of an entity in a sequence so that each position is assigned a unique representation\n",
        "* Multi-Head Attention: self-attention just has one k, v, q and multi-head provides the ability for more relationships in more complex sentences. Translation, language modelling, NER\n",
        "\n",
        "I love **data** *science*\n",
        "\n",
        "and i'm happy that we are here talking about data science\n",
        "\n",
        "Some Transformer Categories:\n",
        "\n",
        "* Encoder Decoder - https://huggingface.co/learn/nlp-course/chapter1/7?fw=pt (summarization, translation, and generative q&a)\n",
        "* Encoder Only - https://huggingface.co/learn/nlp-course/chapter1/5?fw=pt (sentence classification, ner, word classification, and extractive q&a\n",
        "* Decoder Only - https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt (text generation)\n",
        "\n",
        "Sources\n",
        "\n",
        "* https://www.techopedia.com/definition/34116/bidirectional-encoder-representations-from-transformers-bert\n",
        "* https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6_SUo0p4eSt"
      },
      "source": [
        "## Wikipedia\n",
        "\n",
        "### Wikipedia API\n",
        "\n",
        "API stands for Application Programming Interface. In the context of APIs, the word Application refers to any software with a distinct function. Interface can be thought of as a contract of service between two applications. This contract defines how the two communicate with each other using requests and responses.\n",
        "\n",
        "https://aws.amazon.com/what-is/api/\n",
        "\n",
        "If you intend to do any scraping projects or automated requests, consider alternatives such as Pywikipediabot or MediaWiki API, which has other superior features.\n",
        "\n",
        "* wikipedia.search('keywords', results=2)\n",
        "* wikipedia.suggest('keyword')\n",
        "* wikipedia.summary('keywords', sentences=2)\n",
        "* wikipedia.page('keywords')\n",
        "* wikipedia.page('keywords').content\n",
        "* wikipedia.page('keywords').references\n",
        "* wikipedia.page('keywords').title\n",
        "* wikipedia.page('keywords').url\n",
        "* wikipedia.page('keywords').categories\n",
        "* wikipedia.page('keywords').content\n",
        "* wikipedia.page('keywords').links\n",
        "* wikipedia.geosearch(33.2075, 97.1526)\n",
        "* wikipedia.set_lang('hi')\n",
        "* wikipedia.languages()\n",
        "* wikipedia.page('keywords').images[0]\n",
        "* wikipedia.page('keywords').html()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emPkRaJ2aTwJ"
      },
      "source": [
        "## Hugging Face\n",
        "\n",
        "Sources\n",
        "\n",
        "* https://huggingface.co/\n",
        "* https://deeplearningcourses.com/ Data Science: Transformers for Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3BRZIqXaPcJ"
      },
      "source": [
        "### Sentiment Analysis\n",
        "\n",
        "https://huggingface.co/blog/sentiment-analysis-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GIIE5drfdNSH"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers datasets transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RRUBm9CPWiiU"
      },
      "outputs": [],
      "source": [
        "# sentiment analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr3upzNRvyRD"
      },
      "source": [
        "### Text Generation\n",
        "\n",
        "https://huggingface.co/tasks/text-generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iy6LWOssv2DF"
      },
      "outputs": [],
      "source": [
        "# text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoQuYHoWw8mr"
      },
      "source": [
        "Masked Language Modeling\n",
        "\n",
        "https://huggingface.co/tasks/fill-mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vRrU2cECyiZP"
      },
      "outputs": [],
      "source": [
        "# !pip install wikipedia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Roib7jnQyTYx"
      },
      "outputs": [],
      "source": [
        "# masked language model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIlFKQ-q1a8A"
      },
      "source": [
        "### Named Entity Recognition\n",
        "\n",
        "https://huggingface.co/dslim/bert-base-NER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "s9d8JxYa1_YU"
      },
      "outputs": [],
      "source": [
        "# named entity recognition using example = 'I am studying Data Science at the University of North Texas located in Denton, Texas'\n",
        "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "# from transformers import pipeline\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "# model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx0GkLhPeR-P"
      },
      "source": [
        "### Text Summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CYMs57SY36Iu"
      },
      "outputs": [],
      "source": [
        "# text summarization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGcEktBv2uJb"
      },
      "source": [
        "### Translation\n",
        "\n",
        "https://huggingface.co/Helsinki-NLP/opus-mt-en-hi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "H4Pjr46z6Eh0"
      },
      "outputs": [],
      "source": [
        "# translation english to hindi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrVik_Tt2v-4"
      },
      "source": [
        "### Zero Shot Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "DbQLujJ98MV7"
      },
      "outputs": [],
      "source": [
        "# 0 shot example 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hQZbEYdl8rsQ"
      },
      "outputs": [],
      "source": [
        "# 0 shot example 2 using labels [\"cryptography\", \"statistics\", \"mathematics\"]\n",
        "labels = [\"cryptography\", \"statistics\", \"mathematics\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCyFbDYK25ll"
      },
      "source": [
        "### Fine Tuning\n",
        "\n",
        "* Transfer learning is used with transformers because it is practical and cost effective\n",
        "* https://www.tensorflow.org/datasets/catalog/glue"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# data = load_dataset('glue', 'sst2')\n",
        "# print(data)\n",
        "# print(data['train'])\n",
        "# print(data['train'].data)\n",
        "# print(data['train'][0:2])\n",
        "# print(data['train'].features)"
      ],
      "metadata": {
        "id": "yjn-yPdc7c0X"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* GLUE, the General Language Understanding Evaluation benchmark, is a collection of resources for training, evaluating, and analyzing natural language understanding systems.\n",
        "* https://www.tensorflow.org/datasets/catalog/glue"
      ],
      "metadata": {
        "id": "68fTVGdaGAjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dir(data['train'])"
      ],
      "metadata": {
        "id": "qw-OobY6-3eQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, TrainingArguments, AutoModelForSequenceClassification\n",
        "# from pprint import pprint # pretty print\n",
        "\n",
        "# def tokenize_fn(batch):\n",
        "#   return tokenizer(batch['sentence'], truncation=True)\n",
        "\n",
        "# model_version = 'distilbert-base-uncased' # also known as checkpoint\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_version)\n",
        "# sent_tokens = tokenizer(data['train'][0:3]['sentence'])\n",
        "# pprint(sent_tokens)\n",
        "\n",
        "# data_tokens = data.map(tokenize_fn, batched=True)\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#     'my_trainer',\n",
        "#     evaluation_strategy='epoch',\n",
        "#     save_strategy='epoch',\n",
        "#     num_train_epochs=1\n",
        "# )\n",
        "\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     model_version,\n",
        "#     num_labels=2\n",
        "# )\n",
        "\n",
        "# model\n"
      ],
      "metadata": {
        "id": "kJc3HL4-9Wex"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* attention mask - An attention mask is a binary mask that indicates which tokens a model should attend to and which it should ignore. The mask is used when batching sequences together. The model can selectively attend to specific tokens while disregarding others. Bard(2023)\n",
        "* token ids 101 and 102\n",
        "* too many epochs tend to overfit\n",
        "* dropout - method to prevent overfitting, regularization techniques for reducing overfitting in artificial neural networks by preventing complex co-adaptations on training data. They are an efficient way of performing model averaging with neural networks. https://en.wikipedia.org/wiki/Dilution_(neural_networks)\n",
        "\n"
      ],
      "metadata": {
        "id": "sxtAunHsD_E8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install torchinfo"
      ],
      "metadata": {
        "id": "Y8iUIPKD-LxQ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchinfo import summary\n",
        "\n",
        "# summary(model)"
      ],
      "metadata": {
        "id": "qEPejOfpFBQF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save weights before training\n",
        "# weights_before = []\n",
        "# for name, p in model.named_parameters():\n",
        "#   weights_before.append(p.detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "b3BeFwcWFGIT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import Trainer\n",
        "# from datasets import load_metric\n",
        "\n",
        "# metric = load_metric('glue', 'sst2')\n",
        "# print(metric.compute(predictions=[1,0,1], references=[1,0,0]))"
      ],
      "metadata": {
        "id": "FRb5hx4xFovW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_metrics(data):\n",
        "#   vals, labels = data\n",
        "#   predictions = np.argmax(vals, axis=-1)\n",
        "#   return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model,\n",
        "#     training_args,\n",
        "#     train_dataset=data_tokens['train'],\n",
        "#     eval_dataset=data_tokens['validation'],\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=get_metrics,\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "fO9gxIU9HDVO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "fCDSMH1wODYb"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trainer.save_model('saved_model')"
      ],
      "metadata": {
        "id": "9qPaRcnrRzLG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# trained_model = pipeline('text-classification', model='saved_model', device=0)\n",
        "# print(trained_model('this movie is great'))\n",
        "# print(trained_model('this movie is terrible'))"
      ],
      "metadata": {
        "id": "SzcCYgfcR75m"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import json\n",
        "\n",
        "# config_path = 'saved_model/config.json'\n",
        "# with open(config_path) as f:\n",
        "#   j = json.load(f)\n",
        "\n",
        "# j['id2label'] = {0: 'negative', 1: 'positive'} # see Hugging Face AutoConfig\n",
        "\n",
        "# with open(config_path, 'w') as f:\n",
        "#   json.dump(j, f, indent=2)\n",
        "\n",
        "# trained_model = pipeline('text-classification', model='saved_model', device=0)\n",
        "# print(trained_model('this movie is great'))\n",
        "# print(trained_model('this movie is terrible'))\n"
      ],
      "metadata": {
        "id": "T7qxBD29SphM"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# weights_after = []\n",
        "# for name, p in model.named_parameters():\n",
        "#   weights_after.append(p.detach().cpu().numpy())\n",
        "\n",
        "# for w1, w2 in zip(weights_before[0:10], weights_after[0:10]):\n",
        "#   p1 = np.sum(np.abs(w1)).round(2)\n",
        "#   p2 = np.sum(np.abs(w2)).round(2)\n",
        "#   result = np.sum(np.abs(p1 - p2)).round(2)\n",
        "#   print(p1, p2, result)"
      ],
      "metadata": {
        "id": "MgVt_A95Tkbc"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers datasets transformers[torch]"
      ],
      "metadata": {
        "id": "lFqNzpgEmY5-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import seaborn as sn\n",
        "# import matplotlib.pyplot as plt\n",
        "# import torch\n",
        "# from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "# prepare dataset for fine tuning"
      ],
      "metadata": {
        "id": "N9jUoc46iXgK"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from datasets import load_dataset\n",
        "\n",
        "# data = load_dataset('csv', data_files='data.csv')\n",
        "# split = data['train'].train_test_split(test_size=0.3, seed=42)\n",
        "# split"
      ],
      "metadata": {
        "id": "X6mJm0aki-hD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# def tokenize_fn(batch):\n",
        "#   return tokenizer(batch['sentence'], truncation=True)\n",
        "\n",
        "# def compute_metrics(logits_and_labels):\n",
        "#   logits, labels = logits_and_labels\n",
        "#   predictions = np.argmax(logits, axis=-1)\n",
        "#   acc = np.mean(predictions == labels)\n",
        "#   f1 = f1_score(labels, predictions, average='macro')\n",
        "#   return {'accuracy': acc, 'f1': f1}\n",
        "\n",
        "# checkpoint = 'distilbert-base-cased'\n",
        "# tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\n",
        "#     checkpoint,\n",
        "#     num_labels=3\n",
        "#     )\n",
        "\n",
        "# training_args = TrainingArguments(\n",
        "#   output_dir='training_dir',\n",
        "#   evaluation_strategy='epoch',\n",
        "#   save_strategy='epoch',\n",
        "#   num_train_epochs=3,\n",
        "#   per_device_train_batch_size=16,\n",
        "#   per_device_eval_batch_size=64,\n",
        "# )\n",
        "\n",
        "# tokenized_datasets = split.map(tokenize_fn, batched=True)\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model,\n",
        "#     training_args,\n",
        "#     train_dataset=tokenized_datasets[\"train\"],\n",
        "#     eval_dataset=tokenized_datasets[\"test\"],\n",
        "#     tokenizer=tokenizer,\n",
        "#     compute_metrics=compute_metrics,\n",
        "# )\n",
        "\n",
        "# trainer.train()"
      ],
      "metadata": {
        "id": "Vq_c13Rvl439"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls training_dir"
      ],
      "metadata": {
        "id": "skJsp1lfntLw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "\n",
        "# saved_model = pipeline('text-classification', model='training_dir/checkpoint-1282', device=0)\n",
        "# s = split['test']['sentence'][0]\n",
        "# print(s)\n",
        "# saved_model(s)"
      ],
      "metadata": {
        "id": "BPlKzP7Rnjh-"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_label(d):\n",
        "#   return int(d['label'].split('_')[1])\n",
        "\n",
        "# def plot_cm(cm):\n",
        "#   classes = ['negative', 'positive', 'neutral']\n",
        "#   df_cm = pd.DataFrame(cm, index=classes, columns=classes)\n",
        "#   ax = sn.heatmap(df_cm, annot=True, fmt='g')\n",
        "#   ax.set_xlabel('Predicted')\n",
        "#   ax.set_ylabel('Actual')\n",
        "\n",
        "# test_pred = saved_model(split['test']['sentence'])\n",
        "# test_pred = [get_label(d) for d in test_pred]\n",
        "# print('acc:', accuracy_score(split['test']['label'], test_pred))\n",
        "# print('f1:', f1_score(split['test']['label'], test_pred, average='macro'))\n",
        "\n",
        "# cm = confusion_matrix(split['test']['label'], test_pred, normalize='true')\n",
        "# plot_cm(cm)"
      ],
      "metadata": {
        "id": "rEj7roSN23-w"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* when f1 score is less than accuracy then probably the dataset is imbalanced\n",
        "* See Confusion Matrix Multinomial Logistic Regression Notebook"
      ],
      "metadata": {
        "id": "y8ZY9pbE5nbX"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyOsxvhocDYRzSNo4Pf+r90R",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}